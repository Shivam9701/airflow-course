# üìò Apache Airflow Course - Module 2

## üìÇ Module Overview: Exploring Different Types of DAGs

This module explores various DAG patterns, each demonstrating unique features of Airflow. We'll go through each DAG script that you've implemented, explaining its purpose, structure, scheduling, and execution logic in detail. Additionally, we'll include helpful code snippets and real-world use cases to deepen your understanding.

---

## üìÑ 1. `my_first_proper_dag.py`

### ‚úÖ Purpose:

This is your first working DAG. It helps validate that your Airflow installation is functioning correctly and that DAGs are being picked up by the scheduler.

### ‚öôÔ∏è Features:

* Basic Python DAG structure
* Uses the `@dag` decorator and `PythonOperator`

### üß† Key Concepts:

* Importing the required modules: `from airflow import DAG`
* Minimum required arguments for a DAG: `dag_id`, `start_date`, `schedule`, `catchup`
* Use of the `@task` decorator or `PythonOperator` for defining tasks

### üîß Code Snippet:

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(dag_id="my_first_proper_dag", start_date=datetime(2024, 1, 1), schedule="@daily", catchup=False)
def my_first_dag():
    @task
    def say_hello():
        print("Hello, Airflow!")

    say_hello()

my_first_dag()
```

---

## üîó 2. `linear_tasks.py`

### ‚úÖ Purpose:

Demonstrates a simple linear flow between multiple tasks, where each task depends on the successful completion of the previous one.

### ‚öôÔ∏è Features:

* Task 1 ‚Üí Task 2 ‚Üí Task 3 (executed sequentially)
* Explicit task dependencies using `>>` or `<<`

### üß† Key Concepts:

* Linear dependency chains
* Task functions should return or log useful information for better observability

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def task_1():
    print("Task 1 executed")

def task_2():
    print("Task 2 executed")

def task_3():
    print("Task 3 executed")

with DAG(
    dag_id="linear_tasks",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    t1 = PythonOperator(task_id="task_1", python_callable=task_1)
    t2 = PythonOperator(task_id="task_2", python_callable=task_2)
    t3 = PythonOperator(task_id="task_3", python_callable=task_3)

    t1 >> t2 >> t3
```

---

## ‚è±Ô∏è 3. `schedule_5_mins.py`

### ‚úÖ Purpose:

Illustrates how to set up a DAG that runs every 5 minutes using cron-based scheduling.

### ‚öôÔ∏è Features:

* `schedule="*/5 * * * *"` (cron-based scheduling)
* Minimal tasks to observe scheduled triggers

### üß† Key Concepts:

* Cron scheduling in Airflow 3.0 using `schedule` instead of `schedule_interval`
* Use of `start_date` in the past to allow immediate triggering

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def print_message():
    print("This DAG runs every 5 minutes!")

with DAG(
    dag_id="schedule_5_mins",
    start_date=datetime(2024, 1, 1),
    schedule="*/5 * * * *",
    catchup=False,
) as dag:
    task = PythonOperator(task_id="print_message", python_callable=print_message)
```

---

## üåø 4. `branch_dag.py`

### ‚úÖ Purpose:

Demonstrates conditional branching of DAG execution using `BranchPythonOperator`.

### ‚öôÔ∏è Features:

* One task decides which path to follow at runtime
* Followed by conditional downstream tasks

### üß† Key Concepts:

* `BranchPythonOperator` returns the `task_id` of the task to execute
* All downstream tasks must be listed to ensure correct dependency resolution
* Skipped tasks don‚Äôt run (status = skipped)

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime

def decide_branch():
    return "task_a" if datetime.now().minute % 2 == 0 else "task_b"

def task_a():
    print("Task A executed")

def task_b():
    print("Task B executed")

with DAG(
    dag_id="branch_dag",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    branch = BranchPythonOperator(task_id="branch_task", python_callable=decide_branch)
    task_a = PythonOperator(task_id="task_a", python_callable=task_a)
    task_b = PythonOperator(task_id="task_b", python_callable=task_b)

    branch >> [task_a, task_b]
```

---

## üîÅ 5. `basic_retry_dag.py`

### ‚úÖ Purpose:

Showcases automatic retry on task failure.

### ‚öôÔ∏è Features:

* PythonOperator simulating a failure
* DAG-level and task-level retry configurations

### üß† Key Concepts:

* `retries`, `retry_delay`, `max_active_runs`
* Useful for handling transient issues (e.g., network failures)

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def fail_task():
    raise Exception("Simulated failure")

with DAG(
    dag_id="basic_retry_dag",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    default_args={"retries": 3, "retry_delay": timedelta(minutes=5)},
) as dag:
    task = PythonOperator(task_id="fail_task", python_callable=fail_task)
```

---

## üîÑ 6. `fan_out_fan_in_dag.py`

### ‚úÖ Purpose:

Demonstrates the fan-out and fan-in pattern, where multiple tasks run in parallel and then converge back to a single task.

### ‚öôÔ∏è Features:

* Uses `EmptyOperator` for start, join, and end markers
* Parallel execution of tasks using `PythonOperator`
* Converging tasks back to a single join point

### üß† Key Concepts:

* Fan-out: Splitting tasks into parallel execution
* Fan-in: Joining parallel tasks back into a single flow
* Useful for processing multiple datasets or parallel computations

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from datetime import datetime

def log_task(msg):
    print(msg)

with DAG(
    dag_id="fan_out_fan_in_dag",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["fanout", "fanin", "empty_operator", "operator"],
) as dag:

    start = EmptyOperator(task_id="start")

    task1 = PythonOperator(
        task_id="task1",
        python_callable=log_task,
        op_args=["Running Task 1"],
    )

    task2 = PythonOperator(
        task_id="task2",
        python_callable=log_task,
        op_args=["Running Task 2"],
    )

    task3 = PythonOperator(
        task_id="task3",
        python_callable=log_task,
        op_args=["Running Task 3"],
    )

    join = EmptyOperator(task_id="join")
    finish = EmptyOperator(task_id="finish")

    start >> [task1, task2, task3] >> join >> finish
```

---

## üõ†Ô∏è 7. `bash_op.py`

### ‚úÖ Purpose:

Demonstrates the use of `BashOperator` to execute shell commands within a DAG.

### ‚öôÔ∏è Features:

* Executes a simple bash command
* Useful for running shell scripts or system commands

### üß† Key Concepts:

* `BashOperator` bridges Airflow with shell commands
* Ideal for tasks like file manipulation, logging, or triggering external scripts

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id="bash_operator_example",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["bash", "operator", "example"],
) as dag:

    bash_task = BashOperator(
        task_id="print_hello", bash_command="echo 'Hello from BashOperator!'"
    )
```

---

## üß™ 8. `parameterized_dag.py`

### ‚úÖ Purpose:

Illustrates how to make a DAG dynamic using parameters.

### ‚öôÔ∏è Features:

* Uses `params={}` to pass values at runtime
* Uses `dag_run.conf.get()` to retrieve these values in task functions

### üß† Key Concepts:

* Dynamic configuration at trigger time
* Enables creation of more generic DAGs reusable across inputs (e.g., client-specific processing)

### üîß Code Snippet:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def greet(name, age):
    print(f"Hello {name}, you are {age} years old!")

with DAG(
    dag_id="parameterized_dag",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
) as dag:
    greet_task = PythonOperator(
        task_id="greet_person",
        python_callable=greet,
        op_kwargs={"name": "{{ dag_run.conf.get('name', 'Guest') }}", "age": "{{ dag_run.conf.get('age', 0) }}"},
    )
```

---

## ‚úÖ Updated Summary Table

| DAG Name              | Key Feature                   | Operator Type        |
| --------------------- | ----------------------------- | -------------------- |
| `my_first_proper_dag` | Basic DAG Structure           | PythonOperator       |
| `linear_tasks`        | Sequential Dependency Flow    | PythonOperator       |
| `schedule_5_mins`     | Cron-based scheduling         | PythonOperator       |
| `branch_dag`          | Conditional Path Execution    | BranchPythonOperator |
| `basic_retry_dag`     | Retry on Failure              | PythonOperator       |
| `parameterized_dag`   | Parameterized Manual Triggers | PythonOperator       |
| `fan_out_fan_in_dag`  | Fan-Out and Fan-In Pattern    | PythonOperator       |
| `bash_operator`       | Execute Shell Commands        | BashOperator         |

---

## üß≠ Next Module Preview: Operators & Hooks Deep Dive

In the next module, we will explore different Airflow operators (e.g., BashOperator, EmailOperator, PythonOperator), understand hooks and their role in connecting external systems, and learn how to design robust tasks using these building blocks.